{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining/Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving meaningful information from natural language text is known as text analytics or text mining. The goal is to turn text into data for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is the part of computer science and artificial intelligence which deals with human language.some components of NLP given below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.organizing the massive chunks of textual data\n",
    "2.automatic summarization\n",
    "3.machine translation\n",
    "4.named entity recognition\n",
    "5.speech recognition\n",
    "6.topic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of NLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Sentimental analysis - Analysing the given statement either positive or negative or neutral\n",
    "2.Speech recognition - Recognition of speech\n",
    "3.Chatbot - Automatic reply to the customer\n",
    "4.Machine translation - Translation from one language to another\n",
    "5.Spell checking - Checking the spellig for correction\n",
    "6.Keyword search - Searching the keyword inside the textual data\n",
    "7.Information Extraction - Extracting the information from any website\n",
    "8.Advertisement matching - Recommendng the add based on your history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major components of NLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Natural language understanding(NLU) - \n",
    "    Mapping input to useful representations\n",
    "    Analyzing different aspects of the language\n",
    "\n",
    "2.Natural language Generation(NLG) - \n",
    "    Text planning\n",
    "    Sentence Planning\n",
    "    Text Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Ambiguity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Natural Language Understanding\n",
    "\n",
    "1.Lexical ambiguity - Two or more possible meaning of single word\n",
    "    he is looking for a match.\n",
    "    explaination - match may be life partner or cricket match\n",
    "    \n",
    "2.Syntactical ambiguity or Structural ambiguity or grammatical ambiguity - presence of two or more possible meaning within a sentence.\n",
    "    The chicken is ready to eat.\n",
    "    explaination - There is two meaning chicken is ready is food and chicken is ready to eat for us.\n",
    "    \n",
    "3.Referential ambiguity - The presence of two more reference in the sentence.\n",
    "    The boy told his father the theft. he was very upset.\n",
    "    explaination - There are multiple reference of he, may be father, boy, theft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It is a library to building python program to work with natural language data, and it also provides fiftin corpora , we can use it to perform classification, tokenization, steamming, stagging and much more, we can install nltk\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "To download all the corpora you need to open python shell  \n",
    "\n",
    "import nltk \n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different features of NLP that we can perform on text.we will see one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Tokenization is the first step of NLP here we can breaking the complex sentence into words.\n",
    "2.Understanding the importance of the words with respect to sentence.\n",
    "3.produce the structural description of an sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'movie_reviews', 'movie_reviews.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK corpus files for gutenberg\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
     ]
    }
   ],
   "source": [
    "# get the corpus data of 'shakespeare-hamlet.txt' from gutenberg corpus \n",
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "for word in hamlet[:500]:\n",
    "    print(word , end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types in Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "['Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', 'has', 'a', 'design', 'philosophy', 'that', 'emphasizes', 'code', 'readability', ',', 'notably', 'using', 'significant', 'whitespace', '.']\n"
     ]
    }
   ],
   "source": [
    "python = '''\n",
    "Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, \n",
    "Python has a design philosophy that emphasizes code readability, \n",
    "notably using significant whitespace.\n",
    "'''\n",
    "python_tokens = word_tokenize(python)\n",
    "print(len(python_tokens))\n",
    "print(python_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 4,\n",
       "          '.': 2,\n",
       "          '1991': 1,\n",
       "          'a': 1,\n",
       "          'an': 1,\n",
       "          'and': 1,\n",
       "          'by': 1,\n",
       "          'code': 1,\n",
       "          'created': 1,\n",
       "          'design': 1,\n",
       "          'emphasizes': 1,\n",
       "          'first': 1,\n",
       "          'general-purpose': 1,\n",
       "          'guido': 1,\n",
       "          'has': 1,\n",
       "          'high-level': 1,\n",
       "          'in': 1,\n",
       "          'interpreted': 1,\n",
       "          'is': 1,\n",
       "          'language': 1,\n",
       "          'notably': 1,\n",
       "          'philosophy': 1,\n",
       "          'programming': 1,\n",
       "          'python': 2,\n",
       "          'readability': 1,\n",
       "          'released': 1,\n",
       "          'rossum': 1,\n",
       "          'significant': 1,\n",
       "          'that': 1,\n",
       "          'using': 1,\n",
       "          'van': 1,\n",
       "          'whitespace': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency of word in pythton_tokens\n",
    "from nltk.probability import FreqDist\n",
    "word_frequency = FreqDist()\n",
    "\n",
    "for word in python_tokens:\n",
    "    word_frequency[word.lower()]+=1\n",
    "word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['\\nPython is an interpreted, high-level, general-purpose programming language. \\nCreated by Guido van Rossum and first released in 1991, \\nPython has a design philosophy that emphasizes code readability, \\nnotably using significant whitespace.\\n']\n"
     ]
    }
   ],
   "source": [
    "# blank_tokenize to get how many paragraph with new line or how many paragraphs\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "python_blankline_tokens = blankline_tokenize(python)\n",
    "print(len(python_blankline_tokens))\n",
    "print(python_blankline_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['\\nPython is an interpreted, high-level, general-purpose programming language.', 'Created by Guido van Rossum and first released in 1991, \\nPython has a design philosophy that emphasizes code readability, \\nnotably using significant whitespace.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization to get how many sentences in input text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "python_sentence_tokens = sent_tokenize(python)\n",
    "print(len(python_sentence_tokens))\n",
    "print(python_sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Bigrams\n",
    "2.Trigrams\n",
    "3.Ngrams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bigrams - Tokens of two consecutive words\n",
    "Trigrams - Tokens of three consecutive words\n",
    "Ngrams - Tokens of N number of consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[('Python', 'is'), ('is', 'an'), ('an', 'interpreted'), ('interpreted', ','), (',', 'high-level'), ('high-level', ','), (',', 'general-purpose'), ('general-purpose', 'programming'), ('programming', 'language'), ('language', '.'), ('.', 'Created'), ('Created', 'by'), ('by', 'Guido'), ('Guido', 'van'), ('van', 'Rossum'), ('Rossum', 'and'), ('and', 'first'), ('first', 'released'), ('released', 'in'), ('in', '1991'), ('1991', ','), (',', 'Python'), ('Python', 'has'), ('has', 'a'), ('a', 'design'), ('design', 'philosophy'), ('philosophy', 'that'), ('that', 'emphasizes'), ('emphasizes', 'code'), ('code', 'readability'), ('readability', ','), (',', 'notably'), ('notably', 'using'), ('using', 'significant'), ('significant', 'whitespace'), ('whitespace', '.')]\n"
     ]
    }
   ],
   "source": [
    "# BIGRAMS Technique\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "python_tokens_bigrams = list(bigrams(python_tokens))\n",
    "print(len(python_tokens_bigrams))\n",
    "print(python_tokens_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "[('Python', 'is', 'an'), ('is', 'an', 'interpreted'), ('an', 'interpreted', ','), ('interpreted', ',', 'high-level'), (',', 'high-level', ','), ('high-level', ',', 'general-purpose'), (',', 'general-purpose', 'programming'), ('general-purpose', 'programming', 'language'), ('programming', 'language', '.'), ('language', '.', 'Created'), ('.', 'Created', 'by'), ('Created', 'by', 'Guido'), ('by', 'Guido', 'van'), ('Guido', 'van', 'Rossum'), ('van', 'Rossum', 'and'), ('Rossum', 'and', 'first'), ('and', 'first', 'released'), ('first', 'released', 'in'), ('released', 'in', '1991'), ('in', '1991', ','), ('1991', ',', 'Python'), (',', 'Python', 'has'), ('Python', 'has', 'a'), ('has', 'a', 'design'), ('a', 'design', 'philosophy'), ('design', 'philosophy', 'that'), ('philosophy', 'that', 'emphasizes'), ('that', 'emphasizes', 'code'), ('emphasizes', 'code', 'readability'), ('code', 'readability', ','), ('readability', ',', 'notably'), (',', 'notably', 'using'), ('notably', 'using', 'significant'), ('using', 'significant', 'whitespace'), ('significant', 'whitespace', '.')]\n"
     ]
    }
   ],
   "source": [
    "# TRIGRAM Technique\n",
    "python_tokens_trigrams = list(trigrams(python_tokens))\n",
    "print(len(python_tokens_trigrams))\n",
    "print(python_tokens_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "[('Python', 'is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language'), ('is', 'an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.'), ('an', 'interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created'), ('interpreted', ',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by'), (',', 'high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido'), ('high-level', ',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van'), (',', 'general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum'), ('general-purpose', 'programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and'), ('programming', 'language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first'), ('language', '.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released'), ('.', 'Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in'), ('Created', 'by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991'), ('by', 'Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ','), ('Guido', 'van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python'), ('van', 'Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', 'has'), ('Rossum', 'and', 'first', 'released', 'in', '1991', ',', 'Python', 'has', 'a'), ('and', 'first', 'released', 'in', '1991', ',', 'Python', 'has', 'a', 'design'), ('first', 'released', 'in', '1991', ',', 'Python', 'has', 'a', 'design', 'philosophy'), ('released', 'in', '1991', ',', 'Python', 'has', 'a', 'design', 'philosophy', 'that'), ('in', '1991', ',', 'Python', 'has', 'a', 'design', 'philosophy', 'that', 'emphasizes'), ('1991', ',', 'Python', 'has', 'a', 'design', 'philosophy', 'that', 'emphasizes', 'code'), (',', 'Python', 'has', 'a', 'design', 'philosophy', 'that', 'emphasizes', 'code', 'readability'), ('Python', 'has', 'a', 'design', 'philosophy', 'that', 'emphasizes', 'code', 'readability', ','), ('has', 'a', 'design', 'philosophy', 'that', 'emphasizes', 'code', 'readability', ',', 'notably'), ('a', 'design', 'philosophy', 'that', 'emphasizes', 'code', 'readability', ',', 'notably', 'using'), ('design', 'philosophy', 'that', 'emphasizes', 'code', 'readability', ',', 'notably', 'using', 'significant'), ('philosophy', 'that', 'emphasizes', 'code', 'readability', ',', 'notably', 'using', 'significant', 'whitespace'), ('that', 'emphasizes', 'code', 'readability', ',', 'notably', 'using', 'significant', 'whitespace', '.')]\n"
     ]
    }
   ],
   "source": [
    "# NGRAM Technique\n",
    "python_tokens_ngrams = list(ngrams(python_tokens, 10))\n",
    "print(len(python_tokens_ngrams))\n",
    "print(python_tokens_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steamming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the given words in base or root form is known as steaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['affection', 'affect', 'affects', 'affected', 'affecting', 'affections']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer , LancasterStemmer , SnowballStemmer\n",
    "stemmer = PorterStemmer()\n",
    "for word in words:\n",
    "    word = stemmer.stem(word)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LancasterStemmer - It is more aggressive than PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "for word in words:\n",
    "    word = stemmer.stem(word)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SnowballStemmer - where we can perform Steamming on different languages. You need to specify language you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "for word in words:\n",
    "    word = stemmer.stem(word)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological analysis of the words, here we necessary to have a dictionary to analyse the words in which \n",
    "algorithm have look to in the form back to its lemma."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.groups together the different forms of word called lemma.\n",
    "2.somehow similar to stemming as it maps several words into one common root.\n",
    "3.output of the lemmatization is a proper word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example of lemmatization we use the wordnet dictionary and wordNetLeemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affection\n",
      "affect\n",
      "affect\n",
      "affected\n",
      "affecting\n",
      "affection\n"
     ]
    }
   ],
   "source": [
    "# little bit time consuming than stemming because of mapping\n",
    "from nltk.stem import wordnet , WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    print(word)\n",
    "# It will give the result as it is because we not gives the words with pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words is helpful in the creation of sentences but not helpful in the processing of language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['Python', 'interpreted', 'highlevel', 'generalpurpose', 'programming', 'language', 'Created', 'Guido', 'van', 'Rossum', 'first', 'released', '1991', 'Python', 'design', 'philosophy', 'emphasizes', 'code', 'readability', 'notably', 'using', 'significant', 'whitespace']\n"
     ]
    }
   ],
   "source": [
    "python_tokens_without_stopwords = []\n",
    "\n",
    "import re\n",
    "pattern = re.compile('[\\w]{2,200}')\n",
    "for word in python_tokens:\n",
    "    if len(word)>1:\n",
    "        if word not in stop_words:\n",
    "            word = ' '.join(pattern.findall(word)).strip().replace(' ','')\n",
    "            python_tokens_without_stopwords.append(word)\n",
    "print(len(python_tokens_without_stopwords))\n",
    "print(python_tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos_Tagging : Parts of Speech"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NOUN.\n",
    "PRONOUN.\n",
    "VERB.\n",
    "ADJECTIVE.\n",
    "ADVERB.\n",
    "PREPOSITION.\n",
    "CONJUNCTION.\n",
    "INTERJECTION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Python', 'NN')]\n",
      "[('interpreted', 'VBN')]\n",
      "[('highlevel', 'NN')]\n",
      "[('generalpurpose', 'NN')]\n",
      "[('programming', 'VBG')]\n",
      "[('language', 'NN')]\n",
      "[('Created', 'VBN')]\n",
      "[('Guido', 'NN')]\n",
      "[('van', 'NN')]\n",
      "[('Rossum', 'NN')]\n",
      "[('first', 'RB')]\n",
      "[('released', 'VBN')]\n",
      "[('1991', 'CD')]\n",
      "[('Python', 'NN')]\n",
      "[('design', 'NN')]\n",
      "[('philosophy', 'NN')]\n",
      "[('emphasizes', 'NNS')]\n",
      "[('code', 'NN')]\n",
      "[('readability', 'NN')]\n",
      "[('notably', 'RB')]\n",
      "[('using', 'VBG')]\n",
      "[('significant', 'JJ')]\n",
      "[('whitespace', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "for word in python_tokens_without_stopwords:\n",
    "    word = pos_tag([word])\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of detecting the named entity from the text is known as Named Entity Recognition it may be "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Movie\n",
    "2.Money\n",
    "3.Organization\n",
    "4.Location\n",
    "5.Quantities\n",
    "6.Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Python/NNP)\n",
      "  interpreted/VBD\n",
      "  highlevel/NN\n",
      "  generalpurpose/NN\n",
      "  programming/VBG\n",
      "  language/NN\n",
      "  Created/VBD\n",
      "  (PERSON Guido/NNP)\n",
      "  van/NN\n",
      "  (PERSON Rossum/NNP)\n",
      "  first/RB\n",
      "  released/VBD\n",
      "  1991/CD\n",
      "  Python/NNP\n",
      "  design/NN\n",
      "  philosophy/NN\n",
      "  emphasizes/VBZ\n",
      "  code/JJ\n",
      "  readability/NN\n",
      "  notably/RB\n",
      "  using/VBG\n",
      "  significant/JJ\n",
      "  whitespace/NN)\n"
     ]
    }
   ],
   "source": [
    "python = '''\n",
    "Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, \n",
    "Python has a design philosophy that emphasizes code readability, \n",
    "notably using significant whitespace.\n",
    "'''\n",
    "from nltk import ne_chunk\n",
    "python_tokens = word_tokenize(python)\n",
    "python_tokens_with_pos_tag = pos_tag(python_tokens_without_stopwords)\n",
    "python_named_entity = ne_chunk(python_tokens_with_pos_tag)\n",
    "print(python_named_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "syntax means set of rules and principles in given language."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "syntax tree is a representation of syntactic structure of sentences of string.\n",
    "\n",
    "example - The cat sat on mat.\n",
    "\n",
    "where,\n",
    "The - Article\n",
    "cat - Noun\n",
    "sat - verb\n",
    "on - preposition\n",
    "mat - Noun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means picking up single piece of information and grouping them into bigger pieces. This bigger pieces is known as chunk."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As earlier we seen in the Named Entity Recognition how chunking works.\n",
    "In that each word chunking grouped in NN category or PERSON category or GPE category according to pos_tag.\n",
    "\n",
    "example-\n",
    "\n",
    "highlevel/NN\n",
    "generalpurpose/NN\n",
    "\n",
    "PERSON Guido/NNP\n",
    "PERSON Rossum/NNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'movie_reviews', 'movie_reviews.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n",
      "pos/cv000_29590.txt\n",
      "neg/cv000_29416.txt\n"
     ]
    }
   ],
   "source": [
    "positive_reviews = movie_reviews.fileids('pos')\n",
    "negative_reviews = movie_reviews.fileids('neg')\n",
    "print(len(positive_reviews[0]))\n",
    "print(len(negative_reviews[0]))\n",
    "print(positive_reviews[0])\n",
    "print(negative_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success']\n"
     ]
    }
   ],
   "source": [
    "positive = nltk.corpus.movie_reviews.words('pos/cv000_29590.txt')\n",
    "print(list(positive)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'movies', 'like', 'these', 'that', 'make', 'a', 'jaded', 'movie']\n"
     ]
    }
   ],
   "source": [
    "negative = nltk.corpus.movie_reviews.words('neg/cv002_17424.txt')\n",
    "print(list(negative)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "pos/cv000_29590.txt\n",
      "pos/cv001_18431.txt\n",
      "pos/cv002_15918.txt\n",
      "pos/cv003_11664.txt\n",
      "pos/cv004_11636.txt\n",
      "pos/cv005_29443.txt\n",
      "pos/cv006_15448.txt\n",
      "pos/cv007_4968.txt\n",
      "pos/cv008_29435.txt\n",
      "pos/cv009_29592.txt\n",
      "pos/cv010_29198.txt\n",
      "pos/cv011_12166.txt\n",
      "pos/cv012_29576.txt\n",
      "pos/cv013_10159.txt\n",
      "pos/cv014_13924.txt\n",
      "pos/cv015_29439.txt\n",
      "pos/cv016_4659.txt\n",
      "pos/cv017_22464.txt\n",
      "pos/cv018_20137.txt\n",
      "pos/cv019_14482.txt\n",
      "pos/cv020_8825.txt\n",
      "pos/cv021_15838.txt\n",
      "pos/cv022_12864.txt\n",
      "pos/cv023_12672.txt\n",
      "pos/cv024_6778.txt\n",
      "pos/cv025_3108.txt\n",
      "pos/cv026_29325.txt\n",
      "pos/cv027_25219.txt\n",
      "pos/cv028_26746.txt\n",
      "pos/cv029_18643.txt\n",
      "pos/cv030_21593.txt\n",
      "pos/cv031_18452.txt\n",
      "pos/cv032_22550.txt\n",
      "pos/cv033_24444.txt\n",
      "pos/cv034_29647.txt\n",
      "pos/cv035_3954.txt\n",
      "pos/cv036_16831.txt\n",
      "pos/cv037_18510.txt\n",
      "pos/cv038_9749.txt\n",
      "pos/cv039_6170.txt\n",
      "pos/cv040_8276.txt\n",
      "pos/cv041_21113.txt\n",
      "pos/cv042_10982.txt\n",
      "pos/cv043_15013.txt\n",
      "pos/cv044_16969.txt\n",
      "pos/cv045_23923.txt\n",
      "pos/cv046_10188.txt\n",
      "pos/cv047_1754.txt\n",
      "pos/cv048_16828.txt\n",
      "pos/cv049_20471.txt\n",
      "pos/cv050_11175.txt\n",
      "pos/cv051_10306.txt\n",
      "pos/cv052_29378.txt\n",
      "pos/cv053_21822.txt\n",
      "pos/cv054_4230.txt\n",
      "pos/cv055_8338.txt\n",
      "pos/cv056_13133.txt\n",
      "pos/cv057_7453.txt\n",
      "pos/cv058_8025.txt\n",
      "pos/cv059_28885.txt\n",
      "pos/cv060_10844.txt\n",
      "pos/cv061_8837.txt\n",
      "pos/cv062_23115.txt\n",
      "pos/cv063_28997.txt\n",
      "pos/cv064_24576.txt\n",
      "pos/cv065_15248.txt\n",
      "pos/cv066_10821.txt\n",
      "pos/cv067_19774.txt\n",
      "pos/cv068_13400.txt\n",
      "pos/cv069_10801.txt\n",
      "pos/cv070_12289.txt\n",
      "pos/cv071_12095.txt\n",
      "pos/cv072_6169.txt\n",
      "pos/cv073_21785.txt\n",
      "pos/cv074_6875.txt\n",
      "pos/cv075_6500.txt\n",
      "pos/cv076_24945.txt\n",
      "pos/cv077_22138.txt\n",
      "pos/cv078_14730.txt\n",
      "pos/cv079_11933.txt\n",
      "pos/cv080_13465.txt\n",
      "pos/cv081_16582.txt\n",
      "pos/cv082_11080.txt\n",
      "pos/cv083_24234.txt\n",
      "pos/cv084_13566.txt\n",
      "pos/cv085_1381.txt\n",
      "pos/cv086_18371.txt\n",
      "pos/cv087_1989.txt\n",
      "pos/cv088_24113.txt\n",
      "pos/cv089_11418.txt\n",
      "pos/cv090_0042.txt\n",
      "pos/cv091_7400.txt\n",
      "pos/cv092_28017.txt\n",
      "pos/cv093_13951.txt\n",
      "pos/cv094_27889.txt\n",
      "pos/cv095_28892.txt\n",
      "pos/cv096_11474.txt\n",
      "pos/cv097_24970.txt\n",
      "pos/cv098_15435.txt\n",
      "pos/cv099_10534.txt\n",
      "pos/cv100_11528.txt\n",
      "pos/cv101_10175.txt\n",
      "pos/cv102_7846.txt\n",
      "pos/cv103_11021.txt\n",
      "pos/cv104_18134.txt\n",
      "pos/cv105_17990.txt\n",
      "pos/cv106_16807.txt\n",
      "pos/cv107_24319.txt\n",
      "pos/cv108_15571.txt\n",
      "pos/cv109_21172.txt\n",
      "pos/cv110_27788.txt\n",
      "pos/cv111_11473.txt\n",
      "pos/cv112_11193.txt\n",
      "pos/cv113_23102.txt\n",
      "pos/cv114_18398.txt\n",
      "pos/cv115_25396.txt\n",
      "pos/cv116_28942.txt\n",
      "pos/cv117_24295.txt\n",
      "pos/cv118_28980.txt\n",
      "pos/cv119_9867.txt\n",
      "pos/cv120_4111.txt\n",
      "pos/cv121_17302.txt\n",
      "pos/cv122_7392.txt\n",
      "pos/cv123_11182.txt\n",
      "pos/cv124_4122.txt\n",
      "pos/cv125_9391.txt\n",
      "pos/cv126_28971.txt\n",
      "pos/cv127_14711.txt\n",
      "pos/cv128_29627.txt\n",
      "pos/cv129_16741.txt\n",
      "pos/cv130_17083.txt\n",
      "pos/cv131_10713.txt\n",
      "pos/cv132_5618.txt\n",
      "pos/cv133_16336.txt\n",
      "pos/cv134_22246.txt\n",
      "pos/cv135_11603.txt\n",
      "pos/cv136_11505.txt\n",
      "pos/cv137_15422.txt\n",
      "pos/cv138_12721.txt\n",
      "pos/cv139_12873.txt\n",
      "pos/cv140_7479.txt\n",
      "pos/cv141_15686.txt\n",
      "pos/cv142_22516.txt\n",
      "pos/cv143_19666.txt\n",
      "pos/cv144_5007.txt\n",
      "pos/cv145_11472.txt\n",
      "pos/cv146_18458.txt\n",
      "pos/cv147_21193.txt\n",
      "pos/cv148_16345.txt\n",
      "pos/cv149_15670.txt\n",
      "pos/cv150_12916.txt\n",
      "pos/cv151_15771.txt\n",
      "pos/cv152_8736.txt\n",
      "pos/cv153_10779.txt\n",
      "pos/cv154_9328.txt\n",
      "pos/cv155_7308.txt\n",
      "pos/cv156_10481.txt\n",
      "pos/cv157_29372.txt\n",
      "pos/cv158_10390.txt\n",
      "pos/cv159_29505.txt\n",
      "pos/cv160_10362.txt\n",
      "pos/cv161_11425.txt\n",
      "pos/cv162_10424.txt\n",
      "pos/cv163_10052.txt\n",
      "pos/cv164_22447.txt\n",
      "pos/cv165_22619.txt\n",
      "pos/cv166_11052.txt\n",
      "pos/cv167_16376.txt\n",
      "pos/cv168_7050.txt\n",
      "pos/cv169_23778.txt\n",
      "pos/cv170_3006.txt\n",
      "pos/cv171_13537.txt\n",
      "pos/cv172_11131.txt\n",
      "pos/cv173_4471.txt\n",
      "pos/cv174_9659.txt\n",
      "pos/cv175_6964.txt\n",
      "pos/cv176_12857.txt\n",
      "pos/cv177_10367.txt\n",
      "pos/cv178_12972.txt\n",
      "pos/cv179_9228.txt\n",
      "pos/cv180_16113.txt\n",
      "pos/cv181_14401.txt\n",
      "pos/cv182_7281.txt\n",
      "pos/cv183_18612.txt\n",
      "pos/cv184_2673.txt\n",
      "pos/cv185_28654.txt\n",
      "pos/cv186_2269.txt\n",
      "pos/cv187_12829.txt\n",
      "pos/cv188_19226.txt\n",
      "pos/cv189_22934.txt\n",
      "pos/cv190_27052.txt\n",
      "pos/cv191_29719.txt\n",
      "pos/cv192_14395.txt\n",
      "pos/cv193_5416.txt\n",
      "pos/cv194_12079.txt\n",
      "pos/cv195_14528.txt\n",
      "pos/cv196_29027.txt\n",
      "pos/cv197_29328.txt\n",
      "pos/cv198_18180.txt\n",
      "pos/cv199_9629.txt\n",
      "pos/cv200_2915.txt\n",
      "pos/cv201_6997.txt\n",
      "pos/cv202_10654.txt\n",
      "pos/cv203_17986.txt\n",
      "pos/cv204_8451.txt\n",
      "pos/cv205_9457.txt\n",
      "pos/cv206_14293.txt\n",
      "pos/cv207_29284.txt\n",
      "pos/cv208_9020.txt\n",
      "pos/cv209_29118.txt\n",
      "pos/cv210_9312.txt\n",
      "pos/cv211_9953.txt\n",
      "pos/cv212_10027.txt\n",
      "pos/cv213_18934.txt\n",
      "pos/cv214_12294.txt\n",
      "pos/cv215_22240.txt\n",
      "pos/cv216_18738.txt\n",
      "pos/cv217_28842.txt\n",
      "pos/cv218_24352.txt\n",
      "pos/cv219_18626.txt\n",
      "pos/cv220_29059.txt\n",
      "pos/cv221_2695.txt\n",
      "pos/cv222_17395.txt\n",
      "pos/cv223_29066.txt\n",
      "pos/cv224_17661.txt\n",
      "pos/cv225_29224.txt\n",
      "pos/cv226_2618.txt\n",
      "pos/cv227_24215.txt\n",
      "pos/cv228_5806.txt\n",
      "pos/cv229_13611.txt\n",
      "pos/cv230_7428.txt\n",
      "pos/cv231_10425.txt\n",
      "pos/cv232_14991.txt\n",
      "pos/cv233_15964.txt\n",
      "pos/cv234_20643.txt\n",
      "pos/cv235_10217.txt\n",
      "pos/cv236_11565.txt\n",
      "pos/cv237_19221.txt\n",
      "pos/cv238_12931.txt\n",
      "pos/cv239_3385.txt\n",
      "pos/cv240_14336.txt\n",
      "pos/cv241_23130.txt\n",
      "pos/cv242_10638.txt\n",
      "pos/cv243_20728.txt\n",
      "pos/cv244_21649.txt\n",
      "pos/cv245_8569.txt\n",
      "pos/cv246_28807.txt\n",
      "pos/cv247_13142.txt\n",
      "pos/cv248_13987.txt\n",
      "pos/cv249_11640.txt\n",
      "pos/cv250_25616.txt\n",
      "pos/cv251_22636.txt\n",
      "pos/cv252_23779.txt\n",
      "pos/cv253_10077.txt\n",
      "pos/cv254_6027.txt\n",
      "pos/cv255_13683.txt\n",
      "pos/cv256_14740.txt\n",
      "pos/cv257_10975.txt\n",
      "pos/cv258_5792.txt\n",
      "pos/cv259_10934.txt\n",
      "pos/cv260_13959.txt\n",
      "pos/cv261_10954.txt\n",
      "pos/cv262_12649.txt\n",
      "pos/cv263_19259.txt\n",
      "pos/cv264_12801.txt\n",
      "pos/cv265_10814.txt\n",
      "pos/cv266_25779.txt\n",
      "pos/cv267_14952.txt\n",
      "pos/cv268_18834.txt\n",
      "pos/cv269_21732.txt\n",
      "pos/cv270_6079.txt\n",
      "pos/cv271_13837.txt\n",
      "pos/cv272_18974.txt\n",
      "pos/cv273_29112.txt\n",
      "pos/cv274_25253.txt\n",
      "pos/cv275_28887.txt\n",
      "pos/cv276_15684.txt\n",
      "pos/cv277_19091.txt\n",
      "pos/cv278_13041.txt\n",
      "pos/cv279_18329.txt\n",
      "pos/cv280_8267.txt\n",
      "pos/cv281_23253.txt\n",
      "pos/cv282_6653.txt\n",
      "pos/cv283_11055.txt\n",
      "pos/cv284_19119.txt\n",
      "pos/cv285_16494.txt\n",
      "pos/cv286_25050.txt\n",
      "pos/cv287_15900.txt\n",
      "pos/cv288_18791.txt\n",
      "pos/cv289_6463.txt\n",
      "pos/cv290_11084.txt\n",
      "pos/cv291_26635.txt\n",
      "pos/cv292_7282.txt\n",
      "pos/cv293_29856.txt\n",
      "pos/cv294_11684.txt\n",
      "pos/cv295_15570.txt\n",
      "pos/cv296_12251.txt\n",
      "pos/cv297_10047.txt\n",
      "pos/cv298_23111.txt\n",
      "pos/cv299_16214.txt\n",
      "pos/cv300_22284.txt\n",
      "pos/cv301_12146.txt\n",
      "pos/cv302_25649.txt\n",
      "pos/cv303_27520.txt\n",
      "pos/cv304_28706.txt\n",
      "pos/cv305_9946.txt\n",
      "pos/cv306_10364.txt\n",
      "pos/cv307_25270.txt\n",
      "pos/cv308_5016.txt\n",
      "pos/cv309_22571.txt\n",
      "pos/cv310_13091.txt\n",
      "pos/cv311_16002.txt\n",
      "pos/cv312_29377.txt\n",
      "pos/cv313_18198.txt\n",
      "pos/cv314_14422.txt\n",
      "pos/cv315_11629.txt\n",
      "pos/cv316_6370.txt\n",
      "pos/cv317_24049.txt\n",
      "pos/cv318_10493.txt\n",
      "pos/cv319_14727.txt\n",
      "pos/cv320_9530.txt\n",
      "pos/cv321_12843.txt\n",
      "pos/cv322_20318.txt\n",
      "pos/cv323_29805.txt\n",
      "pos/cv324_7082.txt\n",
      "pos/cv325_16629.txt\n",
      "pos/cv326_13295.txt\n",
      "pos/cv327_20292.txt\n",
      "pos/cv328_10373.txt\n",
      "pos/cv329_29370.txt\n",
      "pos/cv330_29809.txt\n",
      "pos/cv331_8273.txt\n",
      "pos/cv332_16307.txt\n",
      "pos/cv333_8916.txt\n",
      "pos/cv334_10001.txt\n",
      "pos/cv335_14665.txt\n",
      "pos/cv336_10143.txt\n",
      "pos/cv337_29181.txt\n",
      "pos/cv338_8821.txt\n",
      "pos/cv339_21119.txt\n",
      "pos/cv340_13287.txt\n",
      "pos/cv341_24430.txt\n",
      "pos/cv342_19456.txt\n",
      "pos/cv343_10368.txt\n",
      "pos/cv344_5312.txt\n",
      "pos/cv345_9954.txt\n",
      "pos/cv346_18168.txt\n",
      "pos/cv347_13194.txt\n",
      "pos/cv348_18176.txt\n",
      "pos/cv349_13507.txt\n",
      "pos/cv350_20670.txt\n",
      "pos/cv351_15458.txt\n",
      "pos/cv352_5524.txt\n",
      "pos/cv353_18159.txt\n",
      "pos/cv354_8132.txt\n",
      "pos/cv355_16413.txt\n",
      "pos/cv356_25163.txt\n",
      "pos/cv357_13156.txt\n",
      "pos/cv358_10691.txt\n",
      "pos/cv359_6647.txt\n",
      "pos/cv360_8398.txt\n",
      "pos/cv361_28944.txt\n",
      "pos/cv362_15341.txt\n",
      "pos/cv363_29332.txt\n",
      "pos/cv364_12901.txt\n",
      "pos/cv365_11576.txt\n",
      "pos/cv366_10221.txt\n",
      "pos/cv367_22792.txt\n",
      "pos/cv368_10466.txt\n",
      "pos/cv369_12886.txt\n",
      "pos/cv370_5221.txt\n",
      "pos/cv371_7630.txt\n",
      "pos/cv372_6552.txt\n",
      "pos/cv373_20404.txt\n",
      "pos/cv374_25436.txt\n",
      "pos/cv375_9929.txt\n",
      "pos/cv376_19435.txt\n",
      "pos/cv377_7946.txt\n",
      "pos/cv378_20629.txt\n",
      "pos/cv379_21963.txt\n",
      "pos/cv380_7574.txt\n",
      "pos/cv381_20172.txt\n",
      "pos/cv382_7897.txt\n",
      "pos/cv383_13116.txt\n",
      "pos/cv384_17140.txt\n",
      "pos/cv385_29741.txt\n",
      "pos/cv386_10080.txt\n",
      "pos/cv387_11507.txt\n",
      "pos/cv388_12009.txt\n",
      "pos/cv389_9369.txt\n",
      "pos/cv390_11345.txt\n",
      "pos/cv391_10802.txt\n",
      "pos/cv392_11458.txt\n",
      "pos/cv393_29327.txt\n",
      "pos/cv394_5137.txt\n",
      "pos/cv395_10849.txt\n",
      "pos/cv396_17989.txt\n",
      "pos/cv397_29023.txt\n",
      "pos/cv398_15537.txt\n",
      "pos/cv399_2877.txt\n",
      "pos/cv400_19220.txt\n",
      "pos/cv401_12605.txt\n",
      "pos/cv402_14425.txt\n",
      "pos/cv403_6621.txt\n",
      "pos/cv404_20315.txt\n",
      "pos/cv405_20399.txt\n",
      "pos/cv406_21020.txt\n",
      "pos/cv407_22637.txt\n",
      "pos/cv408_5297.txt\n",
      "pos/cv409_29786.txt\n",
      "pos/cv410_24266.txt\n",
      "pos/cv411_15007.txt\n",
      "pos/cv412_24095.txt\n",
      "pos/cv413_7398.txt\n",
      "pos/cv414_10518.txt\n",
      "pos/cv415_22517.txt\n",
      "pos/cv416_11136.txt\n",
      "pos/cv417_13115.txt\n",
      "pos/cv418_14774.txt\n",
      "pos/cv419_13394.txt\n",
      "pos/cv420_28795.txt\n",
      "pos/cv421_9709.txt\n",
      "pos/cv422_9381.txt\n",
      "pos/cv423_11155.txt\n",
      "pos/cv424_8831.txt\n",
      "pos/cv425_8250.txt\n",
      "pos/cv426_10421.txt\n",
      "pos/cv427_10825.txt\n",
      "pos/cv428_11347.txt\n",
      "pos/cv429_7439.txt\n",
      "pos/cv430_17351.txt\n",
      "pos/cv431_7085.txt\n",
      "pos/cv432_14224.txt\n",
      "pos/cv433_10144.txt\n",
      "pos/cv434_5793.txt\n",
      "pos/cv435_23110.txt\n",
      "pos/cv436_19179.txt\n",
      "pos/cv437_22849.txt\n",
      "pos/cv438_8043.txt\n",
      "pos/cv439_15970.txt\n",
      "pos/cv440_15243.txt\n",
      "pos/cv441_13711.txt\n",
      "pos/cv442_13846.txt\n",
      "pos/cv443_21118.txt\n",
      "pos/cv444_9974.txt\n",
      "pos/cv445_25882.txt\n",
      "pos/cv446_11353.txt\n",
      "pos/cv447_27332.txt\n",
      "pos/cv448_14695.txt\n",
      "pos/cv449_8785.txt\n",
      "pos/cv450_7890.txt\n",
      "pos/cv451_10690.txt\n",
      "pos/cv452_5088.txt\n",
      "pos/cv453_10379.txt\n",
      "pos/cv454_2053.txt\n",
      "pos/cv455_29000.txt\n",
      "pos/cv456_18985.txt\n",
      "pos/cv457_18453.txt\n",
      "pos/cv458_8604.txt\n",
      "pos/cv459_20319.txt\n",
      "pos/cv460_10842.txt\n",
      "pos/cv461_19600.txt\n",
      "pos/cv462_19350.txt\n",
      "pos/cv463_10343.txt\n",
      "pos/cv464_15650.txt\n",
      "pos/cv465_22431.txt\n",
      "pos/cv466_18722.txt\n",
      "pos/cv467_25773.txt\n",
      "pos/cv468_15228.txt\n",
      "pos/cv469_20630.txt\n",
      "pos/cv470_15952.txt\n",
      "pos/cv471_16858.txt\n",
      "pos/cv472_29280.txt\n",
      "pos/cv473_7367.txt\n",
      "pos/cv474_10209.txt\n",
      "pos/cv475_21692.txt\n",
      "pos/cv476_16856.txt\n",
      "pos/cv477_22479.txt\n",
      "pos/cv478_14309.txt\n",
      "pos/cv479_5649.txt\n",
      "pos/cv480_19817.txt\n",
      "pos/cv481_7436.txt\n",
      "pos/cv482_10580.txt\n",
      "pos/cv483_16378.txt\n",
      "pos/cv484_25054.txt\n",
      "pos/cv485_26649.txt\n",
      "pos/cv486_9799.txt\n",
      "pos/cv487_10446.txt\n",
      "pos/cv488_19856.txt\n",
      "pos/cv489_17906.txt\n",
      "pos/cv490_17872.txt\n",
      "pos/cv491_12145.txt\n",
      "pos/cv492_18271.txt\n",
      "pos/cv493_12839.txt\n",
      "pos/cv494_17389.txt\n",
      "pos/cv495_14518.txt\n",
      "pos/cv496_10530.txt\n",
      "pos/cv497_26980.txt\n",
      "pos/cv498_8832.txt\n",
      "pos/cv499_10658.txt\n",
      "pos/cv500_10251.txt\n",
      "pos/cv501_11657.txt\n",
      "pos/cv502_10406.txt\n",
      "pos/cv503_10558.txt\n",
      "pos/cv504_29243.txt\n",
      "pos/cv505_12090.txt\n",
      "pos/cv506_15956.txt\n",
      "pos/cv507_9220.txt\n",
      "pos/cv508_16006.txt\n",
      "pos/cv509_15888.txt\n",
      "pos/cv510_23360.txt\n",
      "pos/cv511_10132.txt\n",
      "pos/cv512_15965.txt\n",
      "pos/cv513_6923.txt\n",
      "pos/cv514_11187.txt\n",
      "pos/cv515_17069.txt\n",
      "pos/cv516_11172.txt\n",
      "pos/cv517_19219.txt\n",
      "pos/cv518_13331.txt\n",
      "pos/cv519_14661.txt\n",
      "pos/cv520_12295.txt\n",
      "pos/cv521_15828.txt\n",
      "pos/cv522_5583.txt\n",
      "pos/cv523_16615.txt\n",
      "pos/cv524_23627.txt\n",
      "pos/cv525_16122.txt\n",
      "pos/cv526_12083.txt\n",
      "pos/cv527_10123.txt\n",
      "pos/cv528_10822.txt\n",
      "pos/cv529_10420.txt\n",
      "pos/cv530_16212.txt\n",
      "pos/cv531_26486.txt\n",
      "pos/cv532_6522.txt\n",
      "pos/cv533_9821.txt\n",
      "pos/cv534_14083.txt\n",
      "pos/cv535_19728.txt\n",
      "pos/cv536_27134.txt\n",
      "pos/cv537_12370.txt\n",
      "pos/cv538_28667.txt\n",
      "pos/cv539_20347.txt\n",
      "pos/cv540_3421.txt\n",
      "pos/cv541_28835.txt\n",
      "pos/cv542_18980.txt\n",
      "pos/cv543_5045.txt\n",
      "pos/cv544_5108.txt\n",
      "pos/cv545_12014.txt\n",
      "pos/cv546_11767.txt\n",
      "pos/cv547_16324.txt\n",
      "pos/cv548_17731.txt\n",
      "pos/cv549_21443.txt\n",
      "pos/cv550_22211.txt\n",
      "pos/cv551_10565.txt\n",
      "pos/cv552_10016.txt\n",
      "pos/cv553_26915.txt\n",
      "pos/cv554_13151.txt\n",
      "pos/cv555_23922.txt\n",
      "pos/cv556_14808.txt\n",
      "pos/cv557_11449.txt\n",
      "pos/cv558_29507.txt\n",
      "pos/cv559_0050.txt\n",
      "pos/cv560_17175.txt\n",
      "pos/cv561_9201.txt\n",
      "pos/cv562_10359.txt\n",
      "pos/cv563_17257.txt\n",
      "pos/cv564_11110.txt\n",
      "pos/cv565_29572.txt\n",
      "pos/cv566_8581.txt\n",
      "pos/cv567_29611.txt\n",
      "pos/cv568_15638.txt\n",
      "pos/cv569_26381.txt\n",
      "pos/cv570_29082.txt\n",
      "pos/cv571_29366.txt\n",
      "pos/cv572_18657.txt\n",
      "pos/cv573_29525.txt\n",
      "pos/cv574_22156.txt\n",
      "pos/cv575_21150.txt\n",
      "pos/cv576_14094.txt\n",
      "pos/cv577_28549.txt\n",
      "pos/cv578_15094.txt\n",
      "pos/cv579_11605.txt\n",
      "pos/cv580_14064.txt\n",
      "pos/cv581_19381.txt\n",
      "pos/cv582_6559.txt\n",
      "pos/cv583_29692.txt\n",
      "pos/cv584_29722.txt\n",
      "pos/cv585_22496.txt\n",
      "pos/cv586_7543.txt\n",
      "pos/cv587_19162.txt\n",
      "pos/cv588_13008.txt\n",
      "pos/cv589_12064.txt\n",
      "pos/cv590_19290.txt\n",
      "pos/cv591_23640.txt\n",
      "pos/cv592_22315.txt\n",
      "pos/cv593_10987.txt\n",
      "pos/cv594_11039.txt\n",
      "pos/cv595_25335.txt\n",
      "pos/cv596_28311.txt\n",
      "pos/cv597_26360.txt\n",
      "pos/cv598_16452.txt\n",
      "pos/cv599_20988.txt\n",
      "pos/cv600_23878.txt\n",
      "pos/cv601_23453.txt\n",
      "pos/cv602_8300.txt\n",
      "pos/cv603_17694.txt\n",
      "pos/cv604_2230.txt\n",
      "pos/cv605_11800.txt\n",
      "pos/cv606_15985.txt\n",
      "pos/cv607_7717.txt\n",
      "pos/cv608_23231.txt\n",
      "pos/cv609_23877.txt\n",
      "pos/cv610_2287.txt\n",
      "pos/cv611_21120.txt\n",
      "pos/cv612_5461.txt\n",
      "pos/cv613_21796.txt\n",
      "pos/cv614_10626.txt\n",
      "pos/cv615_14182.txt\n",
      "pos/cv616_29319.txt\n",
      "pos/cv617_9322.txt\n",
      "pos/cv618_8974.txt\n",
      "pos/cv619_12462.txt\n",
      "pos/cv620_24265.txt\n",
      "pos/cv621_14368.txt\n",
      "pos/cv622_8147.txt\n",
      "pos/cv623_15356.txt\n",
      "pos/cv624_10744.txt\n",
      "pos/cv625_12440.txt\n",
      "pos/cv626_7410.txt\n",
      "pos/cv627_11620.txt\n",
      "pos/cv628_19325.txt\n",
      "pos/cv629_14909.txt\n",
      "pos/cv630_10057.txt\n",
      "pos/cv631_4967.txt\n",
      "pos/cv632_9610.txt\n",
      "pos/cv633_29837.txt\n",
      "pos/cv634_11101.txt\n",
      "pos/cv635_10022.txt\n",
      "pos/cv636_15279.txt\n",
      "pos/cv637_1250.txt\n",
      "pos/cv638_2953.txt\n",
      "pos/cv639_10308.txt\n",
      "pos/cv640_5378.txt\n",
      "pos/cv641_12349.txt\n",
      "pos/cv642_29867.txt\n",
      "pos/cv643_29349.txt\n",
      "pos/cv644_17154.txt\n",
      "pos/cv645_15668.txt\n",
      "pos/cv646_15065.txt\n",
      "pos/cv647_13691.txt\n",
      "pos/cv648_15792.txt\n",
      "pos/cv649_12735.txt\n",
      "pos/cv650_14340.txt\n",
      "pos/cv651_10492.txt\n",
      "pos/cv652_13972.txt\n",
      "pos/cv653_19583.txt\n",
      "pos/cv654_18246.txt\n",
      "pos/cv655_11154.txt\n",
      "pos/cv656_24201.txt\n",
      "pos/cv657_24513.txt\n",
      "pos/cv658_10532.txt\n",
      "pos/cv659_19944.txt\n",
      "pos/cv660_21893.txt\n",
      "pos/cv661_2450.txt\n",
      "pos/cv662_13320.txt\n",
      "pos/cv663_13019.txt\n",
      "pos/cv664_4389.txt\n",
      "pos/cv665_29538.txt\n",
      "pos/cv666_18963.txt\n",
      "pos/cv667_18467.txt\n",
      "pos/cv668_17604.txt\n",
      "pos/cv669_22995.txt\n",
      "pos/cv670_25826.txt\n",
      "pos/cv671_5054.txt\n",
      "pos/cv672_28083.txt\n",
      "pos/cv673_24714.txt\n",
      "pos/cv674_10732.txt\n",
      "pos/cv675_21588.txt\n",
      "pos/cv676_21090.txt\n",
      "pos/cv677_17715.txt\n",
      "pos/cv678_13419.txt\n",
      "pos/cv679_28559.txt\n",
      "pos/cv680_10160.txt\n",
      "pos/cv681_9692.txt\n",
      "pos/cv682_16139.txt\n",
      "pos/cv683_12167.txt\n",
      "pos/cv684_11798.txt\n",
      "pos/cv685_5947.txt\n",
      "pos/cv686_13900.txt\n",
      "pos/cv687_21100.txt\n",
      "pos/cv688_7368.txt\n",
      "pos/cv689_12587.txt\n",
      "pos/cv690_5619.txt\n",
      "pos/cv691_5043.txt\n",
      "pos/cv692_15451.txt\n",
      "pos/cv693_18063.txt\n",
      "pos/cv694_4876.txt\n",
      "pos/cv695_21108.txt\n",
      "pos/cv696_29740.txt\n",
      "pos/cv697_11162.txt\n",
      "pos/cv698_15253.txt\n",
      "pos/cv699_7223.txt\n",
      "pos/cv700_21947.txt\n",
      "pos/cv701_14252.txt\n",
      "pos/cv702_11500.txt\n",
      "pos/cv703_16143.txt\n",
      "pos/cv704_15969.txt\n",
      "pos/cv705_11059.txt\n",
      "pos/cv706_24716.txt\n",
      "pos/cv707_10678.txt\n",
      "pos/cv708_28729.txt\n",
      "pos/cv709_10529.txt\n",
      "pos/cv710_22577.txt\n",
      "pos/cv711_11665.txt\n",
      "pos/cv712_22920.txt\n",
      "pos/cv713_29155.txt\n",
      "pos/cv714_18502.txt\n",
      "pos/cv715_18179.txt\n",
      "pos/cv716_10514.txt\n",
      "pos/cv717_15953.txt\n",
      "pos/cv718_11434.txt\n",
      "pos/cv719_5713.txt\n",
      "pos/cv720_5389.txt\n",
      "pos/cv721_29121.txt\n",
      "pos/cv722_7110.txt\n",
      "pos/cv723_8648.txt\n",
      "pos/cv724_13681.txt\n",
      "pos/cv725_10103.txt\n",
      "pos/cv726_4719.txt\n",
      "pos/cv727_4978.txt\n",
      "pos/cv728_16133.txt\n",
      "pos/cv729_10154.txt\n",
      "pos/cv730_10279.txt\n",
      "pos/cv731_4136.txt\n",
      "pos/cv732_12245.txt\n",
      "pos/cv733_9839.txt\n",
      "pos/cv734_21568.txt\n",
      "pos/cv735_18801.txt\n",
      "pos/cv736_23670.txt\n",
      "pos/cv737_28907.txt\n",
      "pos/cv738_10116.txt\n",
      "pos/cv739_11209.txt\n",
      "pos/cv740_12445.txt\n",
      "pos/cv741_11890.txt\n",
      "pos/cv742_7751.txt\n",
      "pos/cv743_15449.txt\n",
      "pos/cv744_10038.txt\n",
      "pos/cv745_12773.txt\n",
      "pos/cv746_10147.txt\n",
      "pos/cv747_16556.txt\n",
      "pos/cv748_12786.txt\n",
      "pos/cv749_17765.txt\n",
      "pos/cv750_10180.txt\n",
      "pos/cv751_15719.txt\n",
      "pos/cv752_24155.txt\n",
      "pos/cv753_10875.txt\n",
      "pos/cv754_7216.txt\n",
      "pos/cv755_23616.txt\n",
      "pos/cv756_22540.txt\n",
      "pos/cv757_10189.txt\n",
      "pos/cv758_9671.txt\n",
      "pos/cv759_13522.txt\n",
      "pos/cv760_8597.txt\n",
      "pos/cv761_12620.txt\n",
      "pos/cv762_13927.txt\n",
      "pos/cv763_14729.txt\n",
      "pos/cv764_11739.txt\n",
      "pos/cv765_19037.txt\n",
      "pos/cv766_7540.txt\n",
      "pos/cv767_14062.txt\n",
      "pos/cv768_11751.txt\n",
      "pos/cv769_8123.txt\n",
      "pos/cv770_10451.txt\n",
      "pos/cv771_28665.txt\n",
      "pos/cv772_12119.txt\n",
      "pos/cv773_18817.txt\n",
      "pos/cv774_13845.txt\n",
      "pos/cv775_16237.txt\n",
      "pos/cv776_20529.txt\n",
      "pos/cv777_10094.txt\n",
      "pos/cv778_17330.txt\n",
      "pos/cv779_17881.txt\n",
      "pos/cv780_7984.txt\n",
      "pos/cv781_5262.txt\n",
      "pos/cv782_19526.txt\n",
      "pos/cv783_13227.txt\n",
      "pos/cv784_14394.txt\n",
      "pos/cv785_22600.txt\n",
      "pos/cv786_22497.txt\n",
      "pos/cv787_13743.txt\n",
      "pos/cv788_25272.txt\n",
      "pos/cv789_12136.txt\n",
      "pos/cv790_14600.txt\n",
      "pos/cv791_16302.txt\n",
      "pos/cv792_3832.txt\n",
      "pos/cv793_13650.txt\n",
      "pos/cv794_15868.txt\n",
      "pos/cv795_10122.txt\n",
      "pos/cv796_15782.txt\n",
      "pos/cv797_6957.txt\n",
      "pos/cv798_23531.txt\n",
      "pos/cv799_18543.txt\n",
      "pos/cv800_12368.txt\n",
      "pos/cv801_25228.txt\n",
      "pos/cv802_28664.txt\n",
      "pos/cv803_8207.txt\n",
      "pos/cv804_10862.txt\n",
      "pos/cv805_19601.txt\n",
      "pos/cv806_8842.txt\n",
      "pos/cv807_21740.txt\n",
      "pos/cv808_12635.txt\n",
      "pos/cv809_5009.txt\n",
      "pos/cv810_12458.txt\n",
      "pos/cv811_21386.txt\n",
      "pos/cv812_17924.txt\n",
      "pos/cv813_6534.txt\n",
      "pos/cv814_18975.txt\n",
      "pos/cv815_22456.txt\n",
      "pos/cv816_13655.txt\n",
      "pos/cv817_4041.txt\n",
      "pos/cv818_10211.txt\n",
      "pos/cv819_9364.txt\n",
      "pos/cv820_22892.txt\n",
      "pos/cv821_29364.txt\n",
      "pos/cv822_20049.txt\n",
      "pos/cv823_15569.txt\n",
      "pos/cv824_8838.txt\n",
      "pos/cv825_5063.txt\n",
      "pos/cv826_11834.txt\n",
      "pos/cv827_18331.txt\n",
      "pos/cv828_19831.txt\n",
      "pos/cv829_20289.txt\n",
      "pos/cv830_6014.txt\n",
      "pos/cv831_14689.txt\n",
      "pos/cv832_23275.txt\n",
      "pos/cv833_11053.txt\n",
      "pos/cv834_22195.txt\n",
      "pos/cv835_19159.txt\n",
      "pos/cv836_12968.txt\n",
      "pos/cv837_27325.txt\n",
      "pos/cv838_24728.txt\n",
      "pos/cv839_21467.txt\n",
      "pos/cv840_16321.txt\n",
      "pos/cv841_3967.txt\n",
      "pos/cv842_5866.txt\n",
      "pos/cv843_15544.txt\n",
      "pos/cv844_12690.txt\n",
      "pos/cv845_14290.txt\n",
      "pos/cv846_29497.txt\n",
      "pos/cv847_1941.txt\n",
      "pos/cv848_10036.txt\n",
      "pos/cv849_15729.txt\n",
      "pos/cv850_16466.txt\n",
      "pos/cv851_20469.txt\n",
      "pos/cv852_27523.txt\n",
      "pos/cv853_29233.txt\n",
      "pos/cv854_17740.txt\n",
      "pos/cv855_20661.txt\n",
      "pos/cv856_29013.txt\n",
      "pos/cv857_15958.txt\n",
      "pos/cv858_18819.txt\n",
      "pos/cv859_14107.txt\n",
      "pos/cv860_13853.txt\n",
      "pos/cv861_1198.txt\n",
      "pos/cv862_14324.txt\n",
      "pos/cv863_7424.txt\n",
      "pos/cv864_3416.txt\n",
      "pos/cv865_2895.txt\n",
      "pos/cv866_29691.txt\n",
      "pos/cv867_16661.txt\n",
      "pos/cv868_11948.txt\n",
      "pos/cv869_23611.txt\n",
      "pos/cv870_16348.txt\n",
      "pos/cv871_24888.txt\n",
      "pos/cv872_12591.txt\n",
      "pos/cv873_18636.txt\n",
      "pos/cv874_11236.txt\n",
      "pos/cv875_5754.txt\n",
      "pos/cv876_9390.txt\n",
      "pos/cv877_29274.txt\n",
      "pos/cv878_15694.txt\n",
      "pos/cv879_14903.txt\n",
      "pos/cv880_29800.txt\n",
      "pos/cv881_13254.txt\n",
      "pos/cv882_10026.txt\n",
      "pos/cv883_27751.txt\n",
      "pos/cv884_13632.txt\n",
      "pos/cv885_12318.txt\n",
      "pos/cv886_18177.txt\n",
      "pos/cv887_5126.txt\n",
      "pos/cv888_24435.txt\n",
      "pos/cv889_21430.txt\n",
      "pos/cv890_3977.txt\n",
      "pos/cv891_6385.txt\n",
      "pos/cv892_17576.txt\n",
      "pos/cv893_26269.txt\n",
      "pos/cv894_2068.txt\n",
      "pos/cv895_21022.txt\n",
      "pos/cv896_16071.txt\n",
      "pos/cv897_10837.txt\n",
      "pos/cv898_14187.txt\n",
      "pos/cv899_16014.txt\n",
      "pos/cv900_10331.txt\n",
      "pos/cv901_11017.txt\n",
      "pos/cv902_12256.txt\n",
      "pos/cv903_17822.txt\n",
      "pos/cv904_24353.txt\n",
      "pos/cv905_29114.txt\n",
      "pos/cv906_11491.txt\n",
      "pos/cv907_3541.txt\n",
      "pos/cv908_16009.txt\n",
      "pos/cv909_9960.txt\n",
      "pos/cv910_20488.txt\n",
      "pos/cv911_20260.txt\n",
      "pos/cv912_5674.txt\n",
      "pos/cv913_29252.txt\n",
      "pos/cv914_28742.txt\n",
      "pos/cv915_8841.txt\n",
      "pos/cv916_15467.txt\n",
      "pos/cv917_29715.txt\n",
      "pos/cv918_2693.txt\n",
      "pos/cv919_16380.txt\n",
      "pos/cv920_29622.txt\n",
      "pos/cv921_12747.txt\n",
      "pos/cv922_10073.txt\n",
      "pos/cv923_11051.txt\n",
      "pos/cv924_29540.txt\n",
      "pos/cv925_8969.txt\n",
      "pos/cv926_17059.txt\n",
      "pos/cv927_10681.txt\n",
      "pos/cv928_9168.txt\n",
      "pos/cv929_16908.txt\n",
      "pos/cv930_13475.txt\n",
      "pos/cv931_17563.txt\n",
      "pos/cv932_13401.txt\n",
      "pos/cv933_23776.txt\n",
      "pos/cv934_19027.txt\n",
      "pos/cv935_23841.txt\n",
      "pos/cv936_15954.txt\n",
      "pos/cv937_9811.txt\n",
      "pos/cv938_10220.txt\n",
      "pos/cv939_10583.txt\n",
      "pos/cv940_17705.txt\n",
      "pos/cv941_10246.txt\n",
      "pos/cv942_17082.txt\n",
      "pos/cv943_22488.txt\n",
      "pos/cv944_13521.txt\n",
      "pos/cv945_12160.txt\n",
      "pos/cv946_18658.txt\n",
      "pos/cv947_10601.txt\n",
      "pos/cv948_24606.txt\n",
      "pos/cv949_20112.txt\n",
      "pos/cv950_12350.txt\n",
      "pos/cv951_10926.txt\n",
      "pos/cv952_25240.txt\n",
      "pos/cv953_6836.txt\n",
      "pos/cv954_18628.txt\n",
      "pos/cv955_25001.txt\n",
      "pos/cv956_11609.txt\n",
      "pos/cv957_8737.txt\n",
      "pos/cv958_12162.txt\n",
      "pos/cv959_14611.txt\n",
      "pos/cv960_29007.txt\n",
      "pos/cv961_5682.txt\n",
      "pos/cv962_9803.txt\n",
      "pos/cv963_6895.txt\n",
      "pos/cv964_6021.txt\n",
      "pos/cv965_26071.txt\n",
      "pos/cv966_28832.txt\n",
      "pos/cv967_5788.txt\n",
      "pos/cv968_24218.txt\n",
      "pos/cv969_13250.txt\n",
      "pos/cv970_18450.txt\n",
      "pos/cv971_10874.txt\n",
      "pos/cv972_26417.txt\n",
      "pos/cv973_10066.txt\n",
      "pos/cv974_22941.txt\n",
      "pos/cv975_10981.txt\n",
      "pos/cv976_10267.txt\n",
      "pos/cv977_4938.txt\n",
      "pos/cv978_20929.txt\n",
      "pos/cv979_18921.txt\n",
      "pos/cv980_10953.txt\n",
      "pos/cv981_14989.txt\n",
      "pos/cv982_21103.txt\n",
      "pos/cv983_22928.txt\n",
      "pos/cv984_12767.txt\n",
      "pos/cv985_6359.txt\n",
      "pos/cv986_13527.txt\n",
      "pos/cv987_6965.txt\n",
      "pos/cv988_18740.txt\n",
      "pos/cv989_15824.txt\n",
      "pos/cv990_11591.txt\n",
      "pos/cv991_18645.txt\n",
      "pos/cv992_11962.txt\n",
      "pos/cv993_29737.txt\n",
      "pos/cv994_12270.txt\n",
      "pos/cv995_21821.txt\n",
      "pos/cv996_11592.txt\n",
      "pos/cv997_5046.txt\n",
      "pos/cv998_14111.txt\n",
      "pos/cv999_13106.txt\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_reviews))\n",
    "print(len(negative_reviews))\n",
    "\n",
    "for word in positive_reviews:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the corpus of positive review to review list\n",
    "reviews_list = []\n",
    "for word in positive_reviews:\n",
    "    review = nltk.corpus.movie_reviews.words(word)\n",
    "    review = ' '.join(review)\n",
    "    review = pattern.findall(str(review))\n",
    "    review = ' '.join(review)\n",
    "    reviews_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the corpus of negative review to review list\n",
    "for word in negative_reviews:\n",
    "    review = nltk.corpus.movie_reviews.words(word)\n",
    "    review = ' '.join(review)\n",
    "    review = pattern.findall(str(review))\n",
    "    review = ' '.join(review)\n",
    "    reviews_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_target = np.ones((1000), dtype= np.int)\n",
    "neg_target = np.zeros((1000), dtype = np.int)\n",
    "\n",
    "target_list = list(pos_target) + list(neg_target)\n",
    "len(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(list(zip(reviews_list, target_list)), columns=['reviews','sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every now and then movie comes along from susp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you ve got mail works alot better than it dese...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaws is rare film that grabs your attention be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking is lot like being the general mana...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  films adapted from comic books have had plenty...          1\n",
       "1  every now and then movie comes along from susp...          1\n",
       "2  you ve got mail works alot better than it dese...          1\n",
       "3  jaws is rare film that grabs your attention be...          1\n",
       "4  moviemaking is lot like being the general mana...          1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "x_vector = vectorizer.fit_transform(data.reviews)\n",
    "x_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_vector\n",
    "y = target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39354)\n"
     ]
    }
   ],
   "source": [
    "print(x_vector.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB , MultinomialNB\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 39354)\n",
      "(400, 39354)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "model.predict(x_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = 'movie creater is best person and realistice approached'\n",
    "# statement = 'movie is very bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer.inverse_transform(x_test[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = vectorizer.transform([statement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(new_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if y_pred == [1]:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
